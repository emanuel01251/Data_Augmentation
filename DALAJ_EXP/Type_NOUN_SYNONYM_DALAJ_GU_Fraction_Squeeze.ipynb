{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CtH75gfazCq"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UmMWuow5eCj",
        "outputId": "81da9667-93f3-49ae-cb8f-84c4c30f39f9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'nvidia-smi' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nY9XGIWhTDlI",
        "outputId": "6203fb0b-6dd0-4a6f-b304-d94e24e5e164"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!wget https://svn.spraakdata.gu.se/sb-arkiv/pub/dalaj/datasetDaLAJsplit.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cpMVuMXZUeGX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VRyFdCgvT5bv"
      },
      "outputs": [],
      "source": [
        "dalaj_df = pd.read_csv(\"./content/datasetDaLAJsplit.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "Z9SI5HsXT-X9"
      },
      "outputs": [],
      "source": [
        "#@title Fix arrangement Train\n",
        "\n",
        "train_df = dalaj_df[dalaj_df['split'] == \"train\" ]\n",
        "\n",
        "sorted_items = []\n",
        "for idx, item in train_df.iterrows():\n",
        "  correct_dict = {\"Text\": item[\"corrected sentence\"],\"Label\":1}\n",
        "  sorted_items.append(correct_dict)\n",
        "\n",
        "  incorrect_dict = {\"Text\": item[\"original sentence\"],\"Label\":0}\n",
        "  sorted_items.append(incorrect_dict)\n",
        "\n",
        "final_train_df = pd.DataFrame(sorted_items)\n",
        "from sklearn.utils import shuffle\n",
        "final_train_df = shuffle(final_train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "vEpFBLrPqMup",
        "outputId": "4c2af545-385f-4cff-e2a8-654df9c8e29b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3017</th>\n",
              "      <td>Men i den andra berättelsen , i filmen \" Froze...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6544</th>\n",
              "      <td>Vi har bestämt att du ska gifta dig med Johan .</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>710</th>\n",
              "      <td>Därför väljer skolsystemet att undervisa i des...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>607</th>\n",
              "      <td>Jag tänkte ta med mig mitt barn men det var fö...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7401</th>\n",
              "      <td>T.ex. finns det många fattiga , och de känner ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6447</th>\n",
              "      <td>Sen vill jag anser vilka faktorer som påverkar...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2470</th>\n",
              "      <td>Jag tycker att B-geoplats är en fin plats att ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3701</th>\n",
              "      <td>Jag tror att det i Sverige finns bra möjlighet...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1025</th>\n",
              "      <td>Jag träffar honom verjadag .</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6142</th>\n",
              "      <td>Aspekterna på nackdelar drabbar de som betalar...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7682 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Text  Label\n",
              "3017  Men i den andra berättelsen , i filmen \" Froze...      0\n",
              "6544    Vi har bestämt att du ska gifta dig med Johan .      1\n",
              "710   Därför väljer skolsystemet att undervisa i des...      1\n",
              "607   Jag tänkte ta med mig mitt barn men det var fö...      0\n",
              "7401  T.ex. finns det många fattiga , och de känner ...      0\n",
              "...                                                 ...    ...\n",
              "6447  Sen vill jag anser vilka faktorer som påverkar...      0\n",
              "2470  Jag tycker att B-geoplats är en fin plats att ...      1\n",
              "3701  Jag tror att det i Sverige finns bra möjlighet...      0\n",
              "1025                       Jag träffar honom verjadag .      0\n",
              "6142  Aspekterna på nackdelar drabbar de som betalar...      1\n",
              "\n",
              "[7682 rows x 2 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "AF6ANETnU-ZC"
      },
      "outputs": [],
      "source": [
        "#@title Fix arrangement for test\n",
        "\n",
        "test_df = dalaj_df[dalaj_df['split'] == \"test\" ]\n",
        "\n",
        "sorted_items = []\n",
        "for idx, item in test_df.iterrows():\n",
        "  correct_dict = {\"Text\": item[\"corrected sentence\"],\"Label\":1}\n",
        "  sorted_items.append(correct_dict)\n",
        "\n",
        "  incorrect_dict = {\"Text\": item[\"original sentence\"],\"Label\":0}\n",
        "  sorted_items.append(incorrect_dict)\n",
        "\n",
        "final_test_df = pd.DataFrame(sorted_items)\n",
        "from sklearn.utils import shuffle\n",
        "final_test_df = shuffle(final_test_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "zclE-BjWVCw3",
        "outputId": "639c8353-bbea-48f0-ff33-da76f2f37982"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>474</th>\n",
              "      <td>För pojkarna var det tufft att använda alla ve...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>Hans mamma vill inte intressera sig för honom .</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>575</th>\n",
              "      <td>Du kan också bjuda hem dina grannar för efterm...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>685</th>\n",
              "      <td>För det första så kostar det otroligt mycket p...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>Jag tycker att det är respekt att man ger peng...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>878</th>\n",
              "      <td>Mammorna tog mer ansvar för familjen , och pap...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>871</th>\n",
              "      <td>den Men svåra ekonomiska situationen ledde til...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>653</th>\n",
              "      <td>Jag tror att du förstår mitt problem så att du...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>Det är svårt att lösa situationer där ens kläd...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>468</th>\n",
              "      <td>Det som morfar hade gjorde är ett bra exempel ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>888 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  Text  Label\n",
              "474  För pojkarna var det tufft att använda alla ve...      1\n",
              "172    Hans mamma vill inte intressera sig för honom .      1\n",
              "575  Du kan också bjuda hem dina grannar för efterm...      0\n",
              "685  För det första så kostar det otroligt mycket p...      0\n",
              "301  Jag tycker att det är respekt att man ger peng...      0\n",
              "..                                                 ...    ...\n",
              "878  Mammorna tog mer ansvar för familjen , och pap...      1\n",
              "871  den Men svåra ekonomiska situationen ledde til...      0\n",
              "653  Jag tror att du förstår mitt problem så att du...      0\n",
              "99   Det är svårt att lösa situationer där ens kläd...      0\n",
              "468  Det som morfar hade gjorde är ett bra exempel ...      1\n",
              "\n",
              "[888 rows x 2 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "srh8jiuSU4lM"
      },
      "outputs": [],
      "source": [
        "#@title sentence model\n",
        "#!pip install -U sentence-transformers\n",
        "#from sentence_transformers import SentenceTransformer, models\n",
        "\n",
        "#model_dir = 'Peltarion/xlm-roberta-longformer-base-4096'\n",
        "#mod = \"KB/bert-base-swedish-cased\"\n",
        "#sen_xlmr = \"sentence-transformers/paraphrase-xlm-r-multilingual-v1\"\n",
        "\n",
        "#word_embedding_model = models.Transformer(mod, tokenizer_name_or_path= mod , max_seq_length=512)\n",
        "#pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
        "\n",
        "#model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWv9suyZftF9"
      },
      "source": [
        "#EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "mRqv7ZfGcZst"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Eman\\Downloads\\Text_Aug_Low_Res-main\\Text_Aug_Low_Res-main\\DALAJ_EXP\\swe_aug\\EDA.py:293: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  words = [word for word in words_list if word is not '']  # remove empty words\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "issubclass() arg 1 must be a class",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\" !git clone https://github.com/mosh98/swe_aug.git\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m!pip install -r /content/swe_aug/reqs.txt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m!wget https://www.ida.liu.se/divisions/hcs/nlplab/swectors/swectors-300dim.txt.bz2\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m!bzip2 -dk /content/swectors-300dim.txt.bz2 \"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m word_vec_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./content/swectors-300dim.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mswe_aug\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EDA\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\" aug = EDA.Enkel_Data_Augmentation(word_vec_path) \"\"\"\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Eman\\Downloads\\Text_Aug_Low_Res-main\\Text_Aug_Low_Res-main\\DALAJ_EXP\\swe_aug\\EDA.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy_udpipe\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m shuffle\n",
            "File \u001b[1;32mc:\\Users\\Eman\\Downloads\\Text_Aug_Low_Res-main\\Text_Aug_Low_Res-main\\.venv\\lib\\site-packages\\spacy_udpipe\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UDPipeLanguage, UDPipeModel, load, load_from_path\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download\n\u001b[0;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUDPipeLanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUDPipeModel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_from_path\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownload\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\Eman\\Downloads\\Text_Aug_Low_Res-main\\Text_Aug_Low_Res-main\\.venv\\lib\\site-packages\\spacy_udpipe\\language.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, Iterable, List, Optional, Tuple, Union\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Language\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbols\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEP, HEAD, LEMMA, POS, TAG\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc\n",
            "File \u001b[1;32mc:\\Users\\Eman\\Downloads\\Text_Aug_Low_Res-main\\Text_Aug_Low_Res-main\\.venv\\lib\\site-packages\\spacy\\__init__.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, prefer_gpu, require_cpu, require_gpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Eman\\Downloads\\Text_Aug_Low_Res-main\\Text_Aug_Low_Res-main\\.venv\\lib\\site-packages\\spacy\\pipeline\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattributeruler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AttributeRuler\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdep_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DependencyParser\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01medit_tree_lemmatizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EditTreeLemmatizer\n",
            "File \u001b[1;32mc:\\Users\\Eman\\Downloads\\Text_Aug_Low_Res-main\\Text_Aug_Low_Res-main\\.venv\\lib\\site-packages\\spacy\\pipeline\\attributeruler.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Language\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Matcher\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Scorer\n",
            "File \u001b[1;32mc:\\Users\\Eman\\Downloads\\Text_Aug_Low_Res-main\\Text_Aug_Low_Res-main\\.venv\\lib\\site-packages\\spacy\\language.py:43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BASE_EXCEPTIONS, URL_MATCH\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlookups\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_lookups\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipe_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m analyze_pipes, print_pipe_analysis, validate_attrs\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschemas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     45\u001b[0m     ConfigSchema,\n\u001b[0;32m     46\u001b[0m     ConfigSchemaInit,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m     validate_init_settings,\n\u001b[0;32m     50\u001b[0m )\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Scorer\n",
            "File \u001b[1;32mc:\\Users\\Eman\\Downloads\\Text_Aug_Low_Res-main\\Text_Aug_Low_Res-main\\.venv\\lib\\site-packages\\spacy\\pipe_analysis.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwasabi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m msg\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc, Span, Token\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dot_to_dict\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# This lets us add type hints for mypy etc. without causing circular imports\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Eman\\Downloads\\Text_Aug_Low_Res-main\\Text_Aug_Low_Res-main\\.venv\\lib\\site-packages\\spacy\\tokens\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_serialize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocBin\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmorphanalysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MorphAnalysis\n",
            "File \u001b[1;32mc:\\Users\\Eman\\Downloads\\Text_Aug_Low_Res-main\\Text_Aug_Low_Res-main\\.venv\\lib\\site-packages\\spacy\\tokens\\_serialize.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleFrozenList, ensure_path\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vocab\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dict_proxies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpanGroups\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DOCBIN_ALL_ATTRS \u001b[38;5;28;01mas\u001b[39;00m ALL_ATTRS\n",
            "File \u001b[1;32mc:\\Users\\Eman\\Downloads\\Text_Aug_Low_Res-main\\Text_Aug_Low_Res-main\\.venv\\lib\\site-packages\\spacy\\vocab.pyx:1\u001b[0m, in \u001b[0;36minit spacy.vocab\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\Eman\\Downloads\\Text_Aug_Low_Res-main\\Text_Aug_Low_Res-main\\.venv\\lib\\site-packages\\spacy\\tokens\\doc.pyx:49\u001b[0m, in \u001b[0;36minit spacy.tokens.doc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\Eman\\Downloads\\Text_Aug_Low_Res-main\\Text_Aug_Low_Res-main\\.venv\\lib\\site-packages\\spacy\\schemas.py:287\u001b[0m\n\u001b[0;32m    281\u001b[0m UnderscoreValue \u001b[38;5;241m=\u001b[39m Union[\n\u001b[0;32m    282\u001b[0m     TokenPatternString, TokenPatternNumber, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mbool\u001b[39m\n\u001b[0;32m    283\u001b[0m ]\n\u001b[0;32m    284\u001b[0m IobValue \u001b[38;5;241m=\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m--> 287\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTokenPattern\u001b[39;00m(BaseModel):\n\u001b[0;32m    288\u001b[0m     orth: Optional[StringValue] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     text: Optional[StringValue] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Eman\\Downloads\\Text_Aug_Low_Res-main\\Text_Aug_Low_Res-main\\.venv\\lib\\site-packages\\pydantic\\main.py:299\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[1;34m(mcs, name, bases, namespace, **kwargs)\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    292\u001b[0m         is_untouched(value)\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m ann_type \u001b[38;5;241m!=\u001b[39m PyObject\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         )\n\u001b[0;32m    297\u001b[0m     ):\n\u001b[0;32m    298\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 299\u001b[0m     fields[ann_name] \u001b[38;5;241m=\u001b[39m \u001b[43mModelField\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mann_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mannotation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mann_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_validators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_validators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mann_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ann_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m namespace \u001b[38;5;129;01mand\u001b[39;00m config\u001b[38;5;241m.\u001b[39munderscore_attrs_are_private:\n\u001b[0;32m    307\u001b[0m     private_attributes[ann_name] \u001b[38;5;241m=\u001b[39m PrivateAttr()\n",
            "File \u001b[1;32mc:\\Users\\Eman\\Downloads\\Text_Aug_Low_Res-main\\Text_Aug_Low_Res-main\\.venv\\lib\\site-packages\\pydantic\\fields.py:411\u001b[0m, in \u001b[0;36mModelField.infer\u001b[1;34m(cls, name, value, annotation, class_validators, config)\u001b[0m\n\u001b[0;32m    409\u001b[0m     required \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    410\u001b[0m annotation \u001b[38;5;241m=\u001b[39m get_annotation_from_field_info(annotation, field_info, name, config\u001b[38;5;241m.\u001b[39mvalidate_assignment)\n\u001b[1;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtype_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43malias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_validators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_validators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequired\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequired\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfield_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Eman\\Downloads\\Text_Aug_Low_Res-main\\Text_Aug_Low_Res-main\\.venv\\lib\\site-packages\\pydantic\\fields.py:342\u001b[0m, in \u001b[0;36mModelField.__init__\u001b[1;34m(self, name, type_, class_validators, model_config, default, default_factory, required, alias, field_info)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m SHAPE_SINGLETON\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mprepare_field(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 342\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Eman\\Downloads\\Text_Aug_Low_Res-main\\Text_Aug_Low_Res-main\\.venv\\lib\\site-packages\\pydantic\\fields.py:451\u001b[0m, in \u001b[0;36mModelField.prepare\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m ForwardRef \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m DeferredType:\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;66;03m# self.type_ is currently a ForwardRef and there's nothing we can do now,\u001b[39;00m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;66;03m# user will need to call model.update_forward_refs()\u001b[39;00m\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_type_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequired \u001b[38;5;129;01mis\u001b[39;00m Undefined:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequired \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Eman\\Downloads\\Text_Aug_Low_Res-main\\Text_Aug_Low_Res-main\\.venv\\lib\\site-packages\\pydantic\\fields.py:545\u001b[0m, in \u001b[0;36mModelField._type_analysis\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mouter_type_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;66;03m# re-run to correctly interpret the new self.type_\u001b[39;00m\n\u001b[1;32m--> 545\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_type_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msub_fields \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_sub_type(t, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplay_as_type(t)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m types_]\n",
            "File \u001b[1;32mc:\\Users\\Eman\\Downloads\\Text_Aug_Low_Res-main\\Text_Aug_Low_Res-main\\.venv\\lib\\site-packages\\pydantic\\fields.py:550\u001b[0m, in \u001b[0;36mModelField._type_analysis\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msub_fields \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_sub_type(t, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplay_as_type(t)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m types_]\n\u001b[0;32m    548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43missubclass\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTuple\u001b[49m\u001b[43m)\u001b[49m:  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;66;03m# origin == Tuple without item type\u001b[39;00m\n\u001b[0;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m get_args(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_)\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:  \u001b[38;5;66;03m# plain tuple\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\typing.py:1134\u001b[0m, in \u001b[0;36m_SpecialGenericAlias.__subclasscheck__\u001b[1;34m(self, cls)\u001b[0m\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__origin__, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__origin__)\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mcls\u001b[39m, _GenericAlias):\n\u001b[1;32m-> 1134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43missubclass\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__origin__\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__subclasscheck__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n",
            "\u001b[1;31mTypeError\u001b[0m: issubclass() arg 1 must be a class"
          ]
        }
      ],
      "source": [
        "#@title EDA\n",
        "\"\"\" !git clone https://github.com/mosh98/swe_aug.git\n",
        "!pip install -r /content/swe_aug/reqs.txt\n",
        "!wget https://www.ida.liu.se/divisions/hcs/nlplab/swectors/swectors-300dim.txt.bz2\n",
        "!bzip2 -dk /content/swectors-300dim.txt.bz2 \"\"\"\n",
        "word_vec_path = './content/swectors-300dim.txt'\n",
        "from swe_aug import EDA\n",
        "\"\"\" aug = EDA.Enkel_Data_Augmentation(word_vec_path) \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HKlFNOlfkuq"
      },
      "source": [
        "# Fraction of Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dv5vc1nWDuC"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9xi3sHkPLBv"
      },
      "outputs": [],
      "source": [
        "!pip install MultiEncoder==0.0.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_w2j7PNPOK1"
      },
      "outputs": [],
      "source": [
        "mod = \"KB/bert-base-swedish-cased\"\n",
        "from mle import multi_layer_encoder\n",
        "le = multi_layer_encoder.multi_layer_encoder(mod)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-Sr5gZetf1mF"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def frac(dataframe, fraction,label):\n",
        "    \"\"\"Returns fraction of data\"\"\"\n",
        "    if fraction == 1.0:\n",
        "      return dataframe\n",
        "      \n",
        "    y = dataframe[label]\n",
        "    train, test = train_test_split(dataframe,train_size=fraction,stratify = y)\n",
        "    \n",
        "    return train\n",
        "\n",
        "def encode_df(dataframe, mod=None,col = \"text\"):\n",
        "  encoded = []\n",
        "  for idx, item in dataframe.iterrows():\n",
        "        list_of_encoded_inputs, dect = le.multi_encode(item.Text)\n",
        "        encoded.append(list_of_encoded_inputs[1])\n",
        "        #encoded.append(model.encode(item[col])) \n",
        "\n",
        "  return encoded\n",
        "\n",
        "\n",
        "reports = [] #clf_report (Before Augment, After Augment)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vr9Yj46tqlCw",
        "outputId": "4bfba69d-7dcc-40ba-c519-f6865f5e577e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "test_embed = []\n",
        "\n",
        "y_test = final_test_df.Label\n",
        "\n",
        "for idx, item in final_test_df.iterrows():\n",
        "      list_of_encoded_inputs, dect = le.multi_encode(item.Text)\n",
        "      test_embed.append(list_of_encoded_inputs[1])\n",
        "#test_embed.append(model.encode(item.Text))\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kr3mxNL0fJY",
        "outputId": "4b2152ee-af43-4b8d-cf7e-93e50ce80428"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting SpaceAugmentation\n",
            "  Downloading SpaceAugmentation-0.0.2-py3-none-any.whl (4.6 kB)\n",
            "Installing collected packages: SpaceAugmentation\n",
            "Successfully installed SpaceAugmentation-0.0.2\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "#from swe_aug.Other_Techniques import Text_Cropping\n",
        "#frag = Text_Cropping.cropper(percent = 0.25)\n",
        "!pip install SpaceAugmentation\n",
        "from aug import Augmentation\n",
        "\n",
        "ag = Augmentation.Augmentation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6x7wWhJTZ2xA",
        "outputId": "b87dc599-5de1-4ca0-9b4b-c3931b98ece6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Already downloaded a model for the 'sv' language\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "from swe_aug.Other_Techniques import Type_SR\n",
        "aug = Type_SR.type_DA(word_vec_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU1sg9wXgtMf",
        "outputId": "566b67af-04e0-4d0a-eebf-ed959ecd7c8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "______________________________________________\n",
            "Percentage 10\n",
            "Before Augmentation size:  (768, 2)\n",
            "Before Augmentation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.72      0.62       444\n",
            "           1       0.59      0.40      0.48       444\n",
            "\n",
            "    accuracy                           0.56       888\n",
            "   macro avg       0.57      0.56      0.55       888\n",
            "weighted avg       0.57      0.56      0.55       888\n",
            "\n",
            " \n",
            "After Augmentation size:  1536\n",
            "After Augmentation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.85      0.66       444\n",
            "           1       0.64      0.26      0.37       444\n",
            "\n",
            "    accuracy                           0.56       888\n",
            "   macro avg       0.59      0.56      0.51       888\n",
            "weighted avg       0.59      0.56      0.51       888\n",
            "\n",
            "______________________________________________\n",
            "______________________________________________\n",
            "Percentage 20\n",
            "Before Augmentation size:  (1536, 2)\n",
            "Before Augmentation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.92      0.67       444\n",
            "           1       0.68      0.18      0.28       444\n",
            "\n",
            "    accuracy                           0.55       888\n",
            "   macro avg       0.61      0.55      0.47       888\n",
            "weighted avg       0.61      0.55      0.47       888\n",
            "\n",
            " \n",
            "After Augmentation size:  3072\n",
            "After Augmentation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.54      0.57       444\n",
            "           1       0.58      0.63      0.60       444\n",
            "\n",
            "    accuracy                           0.59       888\n",
            "   macro avg       0.59      0.59      0.59       888\n",
            "weighted avg       0.59      0.59      0.59       888\n",
            "\n",
            "______________________________________________\n",
            "______________________________________________\n",
            "Percentage 30\n",
            "Before Augmentation size:  (2304, 2)\n",
            "Before Augmentation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.74      0.64       444\n",
            "           1       0.62      0.42      0.50       444\n",
            "\n",
            "    accuracy                           0.58       888\n",
            "   macro avg       0.59      0.58      0.57       888\n",
            "weighted avg       0.59      0.58      0.57       888\n",
            "\n",
            " \n",
            "After Augmentation size:  4608\n",
            "After Augmentation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.71      0.64       444\n",
            "           1       0.62      0.48      0.54       444\n",
            "\n",
            "    accuracy                           0.59       888\n",
            "   macro avg       0.60      0.59      0.59       888\n",
            "weighted avg       0.60      0.59      0.59       888\n",
            "\n",
            "______________________________________________\n",
            "______________________________________________\n",
            "Percentage 40\n",
            "Before Augmentation size:  (3072, 2)\n",
            "Before Augmentation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.84      0.66       444\n",
            "           1       0.64      0.28      0.39       444\n",
            "\n",
            "    accuracy                           0.56       888\n",
            "   macro avg       0.59      0.56      0.52       888\n",
            "weighted avg       0.59      0.56      0.52       888\n",
            "\n",
            " \n",
            "After Augmentation size:  6144\n",
            "After Augmentation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.85      0.66       444\n",
            "           1       0.66      0.28      0.40       444\n",
            "\n",
            "    accuracy                           0.57       888\n",
            "   macro avg       0.60      0.57      0.53       888\n",
            "weighted avg       0.60      0.57      0.53       888\n",
            "\n",
            "______________________________________________\n",
            "______________________________________________\n",
            "Percentage 50\n",
            "Before Augmentation size:  (3841, 2)\n",
            "Before Augmentation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.16      0.27       444\n",
            "           1       0.53      0.96      0.69       444\n",
            "\n",
            "    accuracy                           0.56       888\n",
            "   macro avg       0.66      0.56      0.48       888\n",
            "weighted avg       0.66      0.56      0.48       888\n",
            "\n",
            " \n",
            "After Augmentation size:  7681\n",
            "After Augmentation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.60      0.60       444\n",
            "           1       0.60      0.61      0.61       444\n",
            "\n",
            "    accuracy                           0.61       888\n",
            "   macro avg       0.61      0.61      0.61       888\n",
            "weighted avg       0.61      0.61      0.61       888\n",
            "\n",
            "______________________________________________\n",
            "______________________________________________\n",
            "Percentage 60\n",
            "Before Augmentation size:  (4609, 2)\n",
            "Before Augmentation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.48      0.54       444\n",
            "           1       0.58      0.73      0.65       444\n",
            "\n",
            "    accuracy                           0.60       888\n",
            "   macro avg       0.61      0.60      0.59       888\n",
            "weighted avg       0.61      0.60      0.59       888\n",
            "\n",
            " \n",
            "After Augmentation size:  9219\n",
            "After Augmentation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.66      0.63       444\n",
            "           1       0.62      0.56      0.59       444\n",
            "\n",
            "    accuracy                           0.61       888\n",
            "   macro avg       0.61      0.61      0.61       888\n",
            "weighted avg       0.61      0.61      0.61       888\n",
            "\n",
            "______________________________________________\n",
            "______________________________________________\n",
            "Percentage 70\n",
            "Before Augmentation size:  (5377, 2)\n",
            "Before Augmentation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.72      0.66       444\n",
            "           1       0.66      0.56      0.60       444\n",
            "\n",
            "    accuracy                           0.64       888\n",
            "   macro avg       0.64      0.64      0.63       888\n",
            "weighted avg       0.64      0.64      0.63       888\n",
            "\n",
            " \n",
            "After Augmentation size:  10755\n",
            "After Augmentation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.77      0.66       444\n",
            "           1       0.66      0.43      0.52       444\n",
            "\n",
            "    accuracy                           0.60       888\n",
            "   macro avg       0.62      0.60      0.59       888\n",
            "weighted avg       0.62      0.60      0.59       888\n",
            "\n",
            "______________________________________________\n",
            "______________________________________________\n",
            "Percentage 80\n",
            "Before Augmentation size:  (6145, 2)\n",
            "Before Augmentation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.65      0.63       444\n",
            "           1       0.63      0.59      0.61       444\n",
            "\n",
            "    accuracy                           0.62       888\n",
            "   macro avg       0.62      0.62      0.62       888\n",
            "weighted avg       0.62      0.62      0.62       888\n",
            "\n",
            " \n",
            "After Augmentation size:  12291\n",
            "After Augmentation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.39      0.50       444\n",
            "           1       0.58      0.83      0.68       444\n",
            "\n",
            "    accuracy                           0.61       888\n",
            "   macro avg       0.64      0.61      0.59       888\n",
            "weighted avg       0.64      0.61      0.59       888\n",
            "\n",
            "______________________________________________\n",
            "______________________________________________\n",
            "Percentage 90\n",
            "Before Augmentation size:  (6913, 2)\n",
            "Before Augmentation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.12      0.21       444\n",
            "           1       0.52      0.97      0.68       444\n",
            "\n",
            "    accuracy                           0.55       888\n",
            "   macro avg       0.66      0.55      0.45       888\n",
            "weighted avg       0.66      0.55      0.45       888\n",
            "\n",
            " \n",
            "After Augmentation size:  13827\n",
            "After Augmentation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.94      0.68       444\n",
            "           1       0.75      0.19      0.31       444\n",
            "\n",
            "    accuracy                           0.56       888\n",
            "   macro avg       0.64      0.56      0.49       888\n",
            "weighted avg       0.64      0.56      0.49       888\n",
            "\n",
            "______________________________________________\n",
            "______________________________________________\n",
            "Percentage 100\n",
            "Before Augmentation size:  (7682, 2)\n",
            "Before Augmentation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.64      0.64       444\n",
            "           1       0.64      0.64      0.64       444\n",
            "\n",
            "    accuracy                           0.64       888\n",
            "   macro avg       0.64      0.64      0.64       888\n",
            "weighted avg       0.64      0.64      0.64       888\n",
            "\n",
            " \n",
            "After Augmentation size:  15364\n",
            "After Augmentation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.88      0.69       444\n",
            "           1       0.73      0.33      0.45       444\n",
            "\n",
            "    accuracy                           0.60       888\n",
            "   macro avg       0.65      0.60      0.57       888\n",
            "weighted avg       0.65      0.60      0.57       888\n",
            "\n",
            "______________________________________________\n"
          ]
        }
      ],
      "source": [
        "#@title Training Loop\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "split_percentage = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "\n",
        "label = \"Label\"\n",
        "\n",
        "for percentage in split_percentage:\n",
        "    print(\"______________________________________________\")\n",
        "    \n",
        "    print(\"Percentage\",percentage)\n",
        "    \n",
        "    temp_train_df = frac(final_train_df, percentage/100,label)\n",
        "    y_train = temp_train_df.Label\n",
        "    \n",
        "    print(\"Before Augmentation size: \",temp_train_df.shape)\n",
        "    \n",
        "    train_embed = encode_df( temp_train_df,  col=\"Text\")\n",
        "\n",
        "    logreg = SGDClassifier(max_iter=3000,n_jobs=-1)\n",
        "    logreg.fit(train_embed, y_train)\n",
        "    \n",
        "    y_pred = logreg.predict(test_embed)\n",
        "    \n",
        "    r_1 = classification_report(y_test, y_pred)\n",
        "    print(\"Before Augmentation\")\n",
        "    print(r_1)\n",
        "    print(\" \")\n",
        "\n",
        "    # picking out bad samples\n",
        "    incorrect_df = temp_train_df[temp_train_df['Label'] == 0]\n",
        "\n",
        "    #Augmentation\n",
        "\n",
        "    aug_samples = []\n",
        "    for idx, item in incorrect_df.iterrows():\n",
        "      txt = item.Text\n",
        "      lab = item.Label\n",
        "      list_of_augs = aug.type_synonym_sr(txt, token_type = \"NOUN\", n = 2)\n",
        "      \n",
        "      for element in list_of_augs:\n",
        "          aug_samples.append({\"Text\":' '.join(element),\"Label\":lab})\n",
        "\n",
        "    \n",
        "    new_aug_samples = pd.DataFrame(aug_samples)\n",
        "    \n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    new_df = pd.concat([temp_train_df,new_aug_samples],ignore_index=True)\n",
        "    \n",
        "    print(\"After Augmentation size: \",new_df.shape[0])\n",
        "\n",
        "\n",
        "    #augmented train test\n",
        "\n",
        "    #encode\n",
        "    train_embed = encode_df( new_df,  col=\"Text\")\n",
        "    y_train = new_df.Label\n",
        "    \n",
        "    logreg_ = SGDClassifier(max_iter=3000, n_jobs=-1)\n",
        "    logreg_.fit(train_embed, y_train)\n",
        "\n",
        "    y_pred = logreg_.predict(test_embed)\n",
        "\n",
        "\n",
        "    r_2 = classification_report(y_test, y_pred)\n",
        "    print(\"After Augmentation\")\n",
        "    print(r_2)\n",
        "\n",
        "    #save the reports\n",
        "    reports.append((percentage, r_1,r_2))\n",
        "\n",
        "\n",
        "\n",
        "    print(\"______________________________________________\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9akuWLUTxQOw"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -U sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer, models\n",
        "\n",
        "#model_dir = 'Peltarion/xlm-roberta-longformer-base-4096'\n",
        "#mod = \"KB/bert-base-swedish-cased\"\n",
        "sen_xlmr = \"sentence-transformers/paraphrase-xlm-r-multilingual-v1\"\n",
        "\n",
        "word_embedding_model = models.Transformer(sen_xlmr, tokenizer_name_or_path= sen_xlmr , max_seq_length=512)\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
        "\n",
        "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwsA7fUxc-D1"
      },
      "outputs": [],
      "source": [
        "    # picking out bad samples\n",
        "incorrect_df = final_train_df[final_train_df['Label'] == 0]\n",
        "\n",
        "    #Augmentation\n",
        "\n",
        "aug_samples = []\n",
        "original_augmented = [] #every element : orginal_text, (Tuple of augmented sentences)\n",
        "for idx, item in incorrect_df.iterrows():\n",
        "      txt = item.Text\n",
        "      lab = item.Label\n",
        "      list_of_augs = aug.type_synonym_sr(txt, token_type = \"NOUN\", n = 2)\n",
        "      \n",
        "      temp_list = []\n",
        "      for element in list_of_augs:\n",
        "            aug_samples.append({\"Text\":' '.join(element),\"Label\":lab})\n",
        "            temp_list.append(' '.join(element))\n",
        "      \n",
        "      original_augmented.append((txt,temp_list))\n",
        "\n",
        "    \n",
        "\n",
        "new_aug_samples = pd.DataFrame(aug_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRQKDOMPxJ7Y"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "\n",
        "def checkSimilarity(reference_string, list_of_strings, model):\n",
        "    \"\"\"\n",
        "    This function takes a reference string and a list of strings and returns a list of strings that are similar to the reference string.\n",
        "\n",
        "    :param reference_string:\n",
        "    :param list_of_strings:\n",
        "    :param model: sentence transformer model\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    #1. Encode using Sentence Transformer\n",
        "    #2. Calculate cosine similarity\n",
        "    #3. Append the similirty value to the list\n",
        "    # caclulcate percentage of elements below 0.9\n",
        "    # return the list, and the percentage of elements below 0.9\n",
        "\n",
        "    reference_encoded = model.encode(reference_string,convert_to_numpy=True) #encoded reference string\n",
        "    list_of_cosine_similarity = []\n",
        "    for string in list_of_strings:\n",
        "        #print(string)\n",
        "        encoded_string = model.encode(string,convert_to_numpy=True)\n",
        "\n",
        "        #find coside similairity of enocded String and reference string\n",
        "        similarity = util.cos_sim(reference_encoded, encoded_string)\n",
        "        list_of_cosine_similarity.append(similarity.item())\n",
        "    #print(list_of_cosine_similarity)\n",
        "    #sum(i > 5 for i in j)\n",
        "    percentage_of_elements_below_0_9 = sum(i < 0.9 for i in list_of_cosine_similarity) / len(list_of_cosine_similarity)\n",
        "\n",
        "    return sum(i < 0.95 for i in list_of_cosine_similarity),percentage_of_elements_below_0_9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCnYlgH62Cq9"
      },
      "outputs": [],
      "source": [
        "low_semantic_sentences = []\n",
        "for item in original_augmented:\n",
        "  num, percentage = checkSimilarity(item[0],item[1],model)\n",
        "  low_semantic_sentences.append(num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhUhqsjL4Ufe",
        "outputId": "2c2722b1-22d4-4484-c209-95d873d2daf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Sentences: 3841\n",
            "Num of Augmented Sentences: 7682\n"
          ]
        }
      ],
      "source": [
        "print(\"Original Sentences:\",len(original_augmented))\n",
        "print(\"Num of Augmented Sentences:\",len(original_augmented)*2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzn0rs1z5yjZ",
        "outputId": "9cafca69-4b6f-47bc-d2e2-5aca5f33e8b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num of Bad Augmented Sentences: 2782\n"
          ]
        }
      ],
      "source": [
        "print(\"Num of Bad Augmented Sentences:\",sum(low_semantic_sentences))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlioqFao68Ir",
        "outputId": "57ef47c4-8b59-4b71-c6a8-5ecb6d35ed3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Percentage of Bad Augmented Sentences: 36.21452746680552\n"
          ]
        }
      ],
      "source": [
        "num_of_aug_samples = len(original_augmented)*2\n",
        "print(\"Percentage of Bad Augmented Sentences:\",(sum(low_semantic_sentences)/num_of_aug_samples)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K80bYbpg7aha"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMR+5rF1P4fbAqNkJG2Gnot",
      "collapsed_sections": [],
      "mount_file_id": "1uipmJSZ1bfmh-ovt5KIQ0X-tqFJ-rgNq",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
